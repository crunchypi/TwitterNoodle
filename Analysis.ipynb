{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    This file is created for some analysis of the following:\n",
    "        1 - Accuracy of siminets\n",
    "        2 - Accuracy of database usage.\n",
    "    caveat: This will be done with a dataset created by participants\n",
    "    of this project and this dataset is thought to be stronly biased.\n",
    "    \n",
    "    First, Importing modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_tools import dataset_scraper\n",
    "\n",
    "from packages.pipes.collection.feed_disk import FeedFromDiskPipe\n",
    "from packages.pipes.collection.cleaning import CleaningPipe\n",
    "from packages.pipes.collection.simi import SimiPipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    The custom dataset needs to be converted into \"tweet\" objects which\n",
    "    have a functionally similar structure as tweepy tweets.\n",
    "    They are then saved as a list in a pickle file. The reason for doing \n",
    "    this is that it is an easy fromat to handle using the 'pipes' in this project. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242\n"
     ]
    }
   ],
   "source": [
    "# // Load 'tweet' objects.\n",
    "TOPICS = [\"Art\", \"Food\", \"Incident\", \"IT\", \"Nature\", \"Sport\"]  # // Existing topics.\n",
    "PICKLE_FILE = \"./analysis_tools/dummyset.pickle\"\n",
    "\n",
    "dataset_pre_pickle = dataset_scraper.fetch_data_all(\n",
    "    path=\"./analysis_tools/Data\",\n",
    "    topics=TOPICS\n",
    ")\n",
    "\n",
    "# // Check that everything got loaded, should be 242.\n",
    "print(len(dataset_pre_pickle))\n",
    "\n",
    "# // Assign unique user id's. They are functionally used as primary keys in the DB.\n",
    "dataset_scraper.assign_uid(dataset_pre_pickle)\n",
    "\n",
    "# // Pickle\n",
    "dataset_scraper.save(\n",
    "    data=dataset_pre_pickle,\n",
    "    filename=PICKLE_FILE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    At this stage, there should be a dataset at the\n",
    "    path specified above (when doing pickeling). Next cell will\n",
    "    set up three pipes to do the following: \n",
    "         - Fetching the dataset.\n",
    "         - Converting to dataobjects and cleaning.\n",
    "         - Adding siminets to the dataobjects\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242cessed 242 amount of objects\n"
     ]
    }
   ],
   "source": [
    "# // Pipe setup\n",
    "pipe_dsk = FeedFromDiskPipe(\n",
    "    filepath=PICKLE_FILE,\n",
    ")\n",
    "pipe_cln = CleaningPipe(\n",
    "    previous_pipe=pipe_dsk,\n",
    ")\n",
    "pipe_simi = SimiPipe(\n",
    "    previous_pipe=pipe_cln,\n",
    "    recursion_level=2 # Higher is slower and usually more accurate.\n",
    ")\n",
    "pipes = [pipe_dsk, pipe_cln, pipe_simi]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Now that the pipes are set up, they can loop\n",
    "    through their content and process everything.\n",
    "    A thing of note; pipes try to keep their internal\n",
    "    data count (self.output) below a certain amount.\n",
    "    The default is 200 (self.threshold_output),\n",
    "    so the data should be moved somewhere when it's\n",
    "    done (variable below; 'DATA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = [] \n",
    "while True:\n",
    "    for pipe in pipes:\n",
    "        pipe.process()\n",
    "    # // Move data to DATA when it is processed.\n",
    "    if pipe_simi.output: \n",
    "        DATA.append(\n",
    "            pipe_simi.output.pop()\n",
    "        )\n",
    "    else: # Check for break here, doing it in while statement won't work.\n",
    "        break\n",
    "    print(f\"Processed object number: {len(DATA)}\", end=\"\\r\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    As all dataobjects have a similarity net at this stage\n",
    "    some analysis can be done. First part of the analysis\n",
    "    will be by gauging the effectiveness of similarity nets.\n",
    "    \n",
    "    This will require some queries which are converted\n",
    "    into similarity nets. To achieve this, the similarity\n",
    "    tool inside pipe_simi can be borrowed (to avoid\n",
    "    loading it again).\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['metallic', 0.29787667592366535]\n"
     ]
    }
   ],
   "source": [
    "SIMITOOL = pipe_simi.simitool # // For convenience.\n",
    "\n",
    "# // Create dict for queries.\n",
    "QUERY_SIMI = { \n",
    "    topic: SIMITOOL.get_similarity_net(\n",
    "        query=[topic.lower()],\n",
    "        max_recursion=3\n",
    "    ) \n",
    "        for topic in TOPICS\n",
    "}\n",
    "# // Sample a value.\n",
    "print(QUERY_SIMI.get(TOPICS[0])[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    For convenience, all dataobjects in DATA will be sorted\n",
    "    into a dictionary with the following format:\n",
    "        'topic':composite_siminet'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "for dataobj in DATA:\n",
    "    name = dataobj.name\n",
    "    siminet = dataobj.siminet\n",
    "    if name in data_dict:\n",
    "        lst = data_dict[name]\n",
    "        lst.extend(siminet)\n",
    "    else:\n",
    "        data_dict[name] = siminet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    At this stage, the actual result calculation will\n",
    "    be performed. Each siminet of all queries will be\n",
    "    compared against all siminets of dataobjects and\n",
    "    the result will be stored in 'RESULT' with the format: \n",
    "        what_was_searched:dict(compared_topic:score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sport vs  Sport = 9517.9187620877696401707\r"
     ]
    }
   ],
   "source": [
    "# // Create dict of results.\n",
    "RESULT = {}\n",
    "for q_key in QUERY_SIMI:\n",
    "    q_simi = QUERY_SIMI.get(q_key)\n",
    "    \n",
    "    tmp = {} # // Outer dict\n",
    "    for d_key in data_dict:\n",
    "        d_simi = data_dict.get(d_key)\n",
    "        score = SIMITOOL.get_score_compressed_siminet(\n",
    "            new = q_simi,\n",
    "            other = d_simi\n",
    "        )\n",
    "        # // Progress printout for convenience.\n",
    "        print(f\"{q_key} vs {d_key} = {score}\", end='\\r')\n",
    "        tmp[d_key] = score\n",
    "    RESULT[q_key] = tmp # // Add outer dict to inner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    At this stage, the results for siminet accuracy\n",
    "    should be stored in 'RESULT'. The next cell will\n",
    "    do a printout.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query category: 'Art'\n",
      "\t Tweet category ' Art'          got score: '8710.868540873078'\n",
      "\t Tweet category ' Food'         got score: '1345.6881854633502'\n",
      "\t Tweet category ' Incident'     got score: '1355.716669787968'\n",
      "\t Tweet category ' IT'           got score: '2185.552803486575'\n",
      "\t Tweet category ' Nature'       got score: '2021.637268096208'\n",
      "\t Tweet category ' Sport'        got score: '1154.329552670322'\n",
      "Query category: 'Food'\n",
      "\t Tweet category ' Art'          got score: '677.1686447461447'\n",
      "\t Tweet category ' Food'         got score: '7644.890272647085'\n",
      "\t Tweet category ' Incident'     got score: '1202.4390321671979'\n",
      "\t Tweet category ' IT'           got score: '283.3709708849594'\n",
      "\t Tweet category ' Nature'       got score: '1481.1989649434875'\n",
      "\t Tweet category ' Sport'        got score: '308.73552768429164'\n",
      "Query category: 'Incident'\n",
      "\t Tweet category ' Art'          got score: '121.11429974436753'\n",
      "\t Tweet category ' Food'         got score: '146.6687340637046'\n",
      "\t Tweet category ' Incident'     got score: '2530.2579508622407'\n",
      "\t Tweet category ' IT'           got score: '446.57128678758903'\n",
      "\t Tweet category ' Nature'       got score: '470.0962376594545'\n",
      "\t Tweet category ' Sport'        got score: '167.85324875513692'\n",
      "Query category: 'IT'\n",
      "\t Tweet category ' Art'          got score: '9818.211532006771'\n",
      "\t Tweet category ' Food'         got score: '9962.193085342706'\n",
      "\t Tweet category ' Incident'     got score: '11560.904977291935'\n",
      "\t Tweet category ' IT'           got score: '10074.812588572595'\n",
      "\t Tweet category ' Nature'       got score: '8015.066998293025'\n",
      "\t Tweet category ' Sport'        got score: '10694.634524464705'\n",
      "Query category: 'Nature'\n",
      "\t Tweet category ' Art'          got score: '4576.322742571429'\n",
      "\t Tweet category ' Food'         got score: '1345.7387762168987'\n",
      "\t Tweet category ' Incident'     got score: '1661.5573981503667'\n",
      "\t Tweet category ' IT'           got score: '1907.6940122644214'\n",
      "\t Tweet category ' Nature'       got score: '3065.3680917123897'\n",
      "\t Tweet category ' Sport'        got score: '1165.0332695941083'\n",
      "Query category: 'Sport'\n",
      "\t Tweet category ' Art'          got score: '1745.5011356075665'\n",
      "\t Tweet category ' Food'         got score: '1104.7290282547447'\n",
      "\t Tweet category ' Incident'     got score: '1730.5482023557001'\n",
      "\t Tweet category ' IT'           got score: '1849.6734575132566'\n",
      "\t Tweet category ' Nature'       got score: '1361.0409283339964'\n",
      "\t Tweet category ' Sport'        got score: '9517.91876208776'\n"
     ]
    }
   ],
   "source": [
    "# // Calculate padding for printout:\n",
    "padding = 0\n",
    "for key in RESULT:\n",
    "    if len(key) > padding:\n",
    "        padding = len(key)\n",
    "\n",
    "# // Printout\n",
    "for outer_key in RESULT:\n",
    "    print(f\"Query category: '{outer_key}'\")\n",
    "    outer_dict = RESULT.get(outer_key)\n",
    "    for inner_key in outer_dict:\n",
    "        whitespace = padding - len(inner_key) + 5\n",
    "        \n",
    "        print(f\"\\t Tweet category '{inner_key}'\"\n",
    "                 f\"{whitespace*' '} got score: '{outer_dict.get(inner_key)}'\")   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "    Calculating percentages:\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# // Measure percentages:\n",
    "# Topics reminder: TOPICS = [\"Art\", \"Food\", \"IT\", \"Nature\", \"Sport\"]\n",
    "\n",
    "count_dict = {}\n",
    "\n",
    "for item in simi_pipe.output:\n",
    "    score = simi_tool.get_score_compressed_siminet(item.siminet, query_net)\n",
    "    if count_dict.get(item.name):\n",
    "        count_dict[item.name] += score\n",
    "    else:\n",
    "        count_dict[item.name] = score\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
