{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    This file is created for some analysis of the following:\n",
    "        1 - Accuracy of siminets\n",
    "        2 - Accuracy of database usage.\n",
    "    caveat: This will be done with a dataset created by participants\n",
    "    of this project and this dataset is thought to be stronly biased.\n",
    "    \n",
    "    First, Importing modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_tools import dataset_scraper\n",
    "from packages.pipes import prefabs\n",
    "\n",
    "from packages.pipes.collection.feed_disk import FeedFromDiskPipe\n",
    "from packages.pipes.collection.cleaning import CleaningPipe\n",
    "from packages.pipes.collection.simi import SimiPipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    The custom dataset needs to be converted into \"tweet\" objects which\n",
    "    have a functionally similar structure as tweepy tweets.\n",
    "    They are then saved as a list in a pickle file. The reason for doing \n",
    "    this is that it is an easy fromat to handle using the 'pipes' in this project. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242\n"
     ]
    }
   ],
   "source": [
    "# // Load 'tweet' objects.\n",
    "TOPICS = [\"Art\", \"Food\", \"Incident\", \"IT\", \"Nature\", \"Sport\"]  # // Existing topics.\n",
    "PICKLE_FILE = \"./analysis_tools/dummyset.pickle\"\n",
    "\n",
    "dataset_pre_pickle = dataset_scraper.fetch_data_all(\n",
    "    path=\"./analysis_tools/Data\",\n",
    "    topics=TOPICS\n",
    ")\n",
    "\n",
    "# // Check that everything got loaded, should be 242.\n",
    "print(len(dataset_pre_pickle))\n",
    "\n",
    "# // Assign unique user id's. They are functionally used as primary keys in the DB.\n",
    "dataset_scraper.assign_uid(dataset_pre_pickle)\n",
    "\n",
    "# // Pickle\n",
    "dataset_scraper.save(\n",
    "    data=dataset_pre_pickle,\n",
    "    filename=PICKLE_FILE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    At this stage, there should be a dataset at the\n",
    "    path specified above (when doing pickeling). Next cell will\n",
    "    set up three pipes to do the following: \n",
    "         - Fetching the dataset.\n",
    "         - Converting to dataobjects and cleaning.\n",
    "         - Adding siminets to the dataobjects\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# // Pipe setup\n",
    "pipe_dsk = FeedFromDiskPipe(\n",
    "    filepath=PICKLE_FILE,\n",
    ")\n",
    "pipe_cln = CleaningPipe(\n",
    "    previous_pipe=pipe_dsk,\n",
    ")\n",
    "pipe_simi = SimiPipe(\n",
    "    previous_pipe=pipe_cln,\n",
    "    recursion_level=1\n",
    ")\n",
    "pipes = [pipe_dsk, pipe_cln, pipe_simi]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Now that the pipes are set up, they can loop\n",
    "    through their content and process everything.\n",
    "    A thing of note; pipes try to keep their internal\n",
    "    data count (self.output) below a certain amount.\n",
    "    The default is 200 (self.threshold_output),\n",
    "    so the data should be moved somewhere when it's\n",
    "    done (variable below; 'DATA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed object number: 242\r"
     ]
    }
   ],
   "source": [
    "DATA = [] \n",
    "while True:\n",
    "    for pipe in pipes:\n",
    "        pipe.process()\n",
    "    # // Move data to DATA when it is processed.\n",
    "    if pipe_simi.output: \n",
    "        DATA.append(\n",
    "            pipe_simi.output.pop()\n",
    "        )\n",
    "    else: # Check for break here, doing it in while statement won't work.\n",
    "        break\n",
    "    print(f\"Processed object number: {len(DATA)}\", end=\"\\r\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    As all dataobjects have a similarity net at this stage\n",
    "    some analysis can be done. First part of the analysis\n",
    "    will be by gauging the effectiveness of similarity nets.\n",
    "    \n",
    "    This will require some queries which are converted\n",
    "    into similarity nets. To achieve this, the similarity\n",
    "    tool inside pipe_simi can be borrowed (to avoid\n",
    "    loading it again).\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['collection', 0.8777222037315369]\n"
     ]
    }
   ],
   "source": [
    "SIMITOOL = pipe_simi.simitool # // For convenience.\n",
    "\n",
    "# // Create dict for queries.\n",
    "QUERY_SIMI = { \n",
    "    topic: SIMITOOL.get_similarity_net(\n",
    "        query=[topic.lower()],\n",
    "        max_recursion=1\n",
    "    ) \n",
    "        for topic in TOPICS\n",
    "}\n",
    "# // Sample a value.\n",
    "print(QUERY_SIMI.get(TOPICS[0])[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    For convenience, all dataobjects in DATA will be sorted\n",
    "    into a dictionary with the following format:\n",
    "        'topic':composite_siminet'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "for dataobj in DATA:\n",
    "    name = dataobj.name[1:] # Bug correction; names start with a space\n",
    "    siminet = dataobj.siminet\n",
    "    if name in data_dict:\n",
    "        lst = data_dict[name]\n",
    "        lst.extend(siminet)\n",
    "    else:\n",
    "        data_dict[name] = siminet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    At this stage, the actual result calculation will\n",
    "    be performed. Each siminet of all queries will be\n",
    "    compared against all siminets of dataobjects and\n",
    "    the result will be stored in 'RESULT' with the format: \n",
    "        what_was_searched:dict(compared_topic:score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sport vs Sport = 115.8812288045883282363\r"
     ]
    }
   ],
   "source": [
    "# // Create dict of results.\n",
    "RESULT = {}\n",
    "for q_key in QUERY_SIMI:\n",
    "    q_simi = QUERY_SIMI.get(q_key)\n",
    "    \n",
    "    tmp = {} # // Outer dict\n",
    "    for d_key in data_dict:\n",
    "        d_simi = data_dict.get(d_key)\n",
    "        score = SIMITOOL.get_score_compressed_siminet(\n",
    "            new = q_simi,\n",
    "            other = d_simi\n",
    "        )\n",
    "        # // Progress printout for convenience.\n",
    "        print(f\"{q_key} vs {d_key} = {score}\", end='\\r')\n",
    "        tmp[d_key] = score\n",
    "    RESULT[q_key] = tmp # // Add outer dict to inner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    At this stage, the results for siminet accuracy\n",
    "    should be stored in 'RESULT'. The next cell will\n",
    "    do a printout.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query category: 'Art'\n",
      "\t Tweet category 'Art'           got score: '407.8573306798935'\n",
      "\t Tweet category 'Food'          got score: '0'\n",
      "\t Tweet category 'Incident'      got score: '0'\n",
      "\t Tweet category 'IT'            got score: '1.7995017170906067'\n",
      "\t Tweet category 'Nature'        got score: '12.358308136463165'\n",
      "\t Tweet category 'Sport'         got score: '1.7833257913589478'\n",
      "Query category: 'Food'\n",
      "\t Tweet category 'Art'           got score: '0'\n",
      "\t Tweet category 'Food'          got score: '148.80612349510193'\n",
      "\t Tweet category 'Incident'      got score: '0'\n",
      "\t Tweet category 'IT'            got score: '0'\n",
      "\t Tweet category 'Nature'        got score: '5.274604618549347'\n",
      "\t Tweet category 'Sport'         got score: '0'\n",
      "Query category: 'Incident'\n",
      "\t Tweet category 'Art'           got score: '0'\n",
      "\t Tweet category 'Food'          got score: '0'\n",
      "\t Tweet category 'Incident'      got score: '44.58121180534363'\n",
      "\t Tweet category 'IT'            got score: '0'\n",
      "\t Tweet category 'Nature'        got score: '0'\n",
      "\t Tweet category 'Sport'         got score: '0'\n",
      "Query category: 'IT'\n",
      "\t Tweet category 'Art'           got score: '77.6699651479721'\n",
      "\t Tweet category 'Food'          got score: '84.256707072258'\n",
      "\t Tweet category 'Incident'      got score: '154.76139879226685'\n",
      "\t Tweet category 'IT'            got score: '111.46057051420212'\n",
      "\t Tweet category 'Nature'        got score: '58.66206282377243'\n",
      "\t Tweet category 'Sport'         got score: '80.68214333057404'\n",
      "Query category: 'Nature'\n",
      "\t Tweet category 'Art'           got score: '42.35894572734833'\n",
      "\t Tweet category 'Food'          got score: '0'\n",
      "\t Tweet category 'Incident'      got score: '0'\n",
      "\t Tweet category 'IT'            got score: '1.7132743000984192'\n",
      "\t Tweet category 'Nature'        got score: '117.71046948432922'\n",
      "\t Tweet category 'Sport'         got score: '0'\n",
      "Query category: 'Sport'\n",
      "\t Tweet category 'Art'           got score: '1.7489410042762756'\n",
      "\t Tweet category 'Food'          got score: '0'\n",
      "\t Tweet category 'Incident'      got score: '0'\n",
      "\t Tweet category 'IT'            got score: '7.086031794548035'\n",
      "\t Tweet category 'Nature'        got score: '1.7223346829414368'\n",
      "\t Tweet category 'Sport'         got score: '115.88122880458832'\n"
     ]
    }
   ],
   "source": [
    "# // Calculate padding for printout:\n",
    "padding = 0\n",
    "for key in RESULT:\n",
    "    if len(key) > padding:\n",
    "        padding = len(key)\n",
    "\n",
    "# // Printout\n",
    "for outer_key in RESULT:\n",
    "    print(f\"Query category: '{outer_key}'\")\n",
    "    outer_dict = RESULT.get(outer_key)\n",
    "    for inner_key in outer_dict:\n",
    "        whitespace = padding - len(inner_key) + 5\n",
    "        \n",
    "        print(f\"\\t Tweet category '{inner_key}'\"\n",
    "                 f\"{whitespace*' '} got score: '{outer_dict.get(inner_key)}'\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    -----------\n",
    "    As all values have been gathered, some percentages can be\n",
    "    shown as well.\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Art : 0.962385103034575\n",
      "Food : 0.9657672657500764\n",
      "Incident : 1.0\n",
      "IT : 0.19640876703497692\n",
      "Nature : 0.7275838338425028\n",
      "Sport : 0.9165024541421779\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# // Collection.\n",
    "result_percent = {}\n",
    "for o_key in RESULT:\n",
    "    inner_dict = RESULT.get(o_key)\n",
    "    # // Sum all.\n",
    "    inner_total = 0\n",
    "    for i_key in inner_dict:\n",
    "        inner_total += inner_dict.get(i_key)\n",
    "    percent = (inner_dict.get(o_key) / inner_total)\n",
    "    # // Save.\n",
    "    result_percent[o_key] = percent\n",
    "    \n",
    "# // Printout.\n",
    "for key in result_percent:\n",
    "    print(f\"{key} : {result_percent.get(key)}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    -----------\n",
    "    Conclusion\n",
    "    ----------\n",
    "    After testing with recursion levels 1-2(inclusive) on dataobject siminet\n",
    "    and 1-3(also inclusive), there are a few remarks to be made:\n",
    "    \n",
    "    NOTE: All stats were calculated with a float cut-off after second decimal.\n",
    "    \n",
    "    The absolute worst performance was with the category 'IT', while\n",
    "    the absolute best performance was with the category 'Incident'.\n",
    "    This was a recurring pattern through all tests.\n",
    "        Absolute worst; 15% with 'IT'\n",
    "        Absolute best; 100% with 'Incident' (floating point cut-off - actually 99.x)\n",
    "    \n",
    "        Average with obj rec 1 query req 1: 76%\n",
    "        Average with obj rec 1 query req 2: 67%\n",
    "        Average with obj rec 1 query req 3: 53%\n",
    "        \n",
    "        Average with obj rec 2 query req 1: 69%\n",
    "        Average with obj rec 2 query req 2: 60%\n",
    "        Average with obj rec 2 query req 3: 45%\n",
    "        \n",
    "    \n",
    "    \n",
    "    More details:\n",
    "    \n",
    "            Art, Food, Incident, IT, Nature, Sport (columns)\n",
    "    \n",
    "        rec obj 1, rec query 1\n",
    "\t\n",
    "            0.96 + 0.96 + 1 + 0.19 + 0.72 + 0.9 = 4.73\n",
    "            4.73 / 6 = 0.78\n",
    "    \n",
    "        rec obj 1, rec query 2\n",
    "\t\n",
    "            0.82 + 0.89 + 0.94 + 0.15 + 0.38 + 0.87 = 4.05\n",
    "            4.05 / 6 = 0.67\n",
    "\n",
    "        rec obj 1, rec query 3\n",
    "\n",
    "            0.65 + 0.79 + 0.72 + 0.15 + 0.23 + 0.67 = 3.21\n",
    "            3.21 / 6 = 0.53\n",
    "\n",
    "        rec obj 2, rec query 1\n",
    "\n",
    "            0.85 + 0.86 + 0.98 + 0.15 + 0.5 + 0.8 = 4.14\n",
    "            4.14 / 6 = 0.69\n",
    "\n",
    "        rec obj 2, rec query 2\n",
    "\n",
    "            0.75 + 0.76 + 0.86 + 0.16 + 0.36 + 0.74 = 3.63\n",
    "            3.63 / 6 = 0.60\n",
    "\n",
    "        rec obj 3, rec query 3\n",
    "\n",
    "            0.52 + 0.66 + 0.64 + 0.16 + 0.22 + 0.54 = 2.74\n",
    "            2.74 / 6 = 0.45\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ---------------------------------------------------------------\n",
    "  ---------------------------------------------------------------\n",
    "  ---------------------------------------------------------------\n",
    "  ---------------------------------------------------------------\n",
    "  ---------------------------------------------------------------\n",
    "  ---------------------------------------------------------------\n",
    "\n",
    "            This section is for inserting the dataobjects \n",
    "            created above into neo4j. It is important to\n",
    "            have all credentials in order (credentials file)\n",
    "            and start the server.\n",
    "\n",
    "            First, the pickle file will be saved again, with\n",
    "            fever objects. This is because it is difficult to\n",
    "            see what's going on in the database with more than\n",
    "            200 nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# // Load 'tweet' objects.\n",
    "TOPICS = [\"Incident\", \"IT\"]  # // Existing topics.\n",
    "PICKLE_FILE = \"./analysis_tools/dummyset.pickle\"\n",
    "\n",
    "\n",
    "\n",
    "incident_tweets = dataset_scraper.fetch_data_by_topic(\n",
    "    path=\"./analysis_tools/Data\",\n",
    "    topic=\"Incident\"\n",
    ")\n",
    "\n",
    "it_tweets = dataset_scraper.fetch_data_by_topic(\n",
    "    path=\"./analysis_tools/Data\",\n",
    "    topic=\"IT\"\n",
    ")\n",
    "\n",
    "\n",
    "# // Reduce count.\n",
    "incident_tweets = incident_tweets[int(len(incident_tweets)/2):]\n",
    "it_tweets = it_tweets[int(len(it_tweets)/2):]\n",
    "\n",
    "# // Check that everything got loaded, should be 242.\n",
    "print(len(incident_tweets))\n",
    "print(len(it_tweets))\n",
    "\n",
    "# # // Assign unique user id's. They are functionally used as primary keys in the DB.\n",
    "dataset_scraper.assign_uid(incident_tweets)\n",
    "dataset_scraper.assign_uid(it_tweets)\n",
    "\n",
    "# // Combine before save.\n",
    "combined = []\n",
    "combined.extend(incident_tweets)\n",
    "combined.extend(it_tweets)\n",
    "\n",
    "# // Pickle\n",
    "dataset_scraper.save(\n",
    "    data=combined,\n",
    "    filename=PICKLE_FILE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            The rest is just running the pipe. Note:\n",
    "                - There's currently an issue with quitting the pipe,\n",
    "                  it has to crash.\n",
    "                - There's a default node count limit (25 nodes) when \n",
    "                  viewing the databse (at localhost:7474). This can\n",
    "                  be fixed by typing this into the command:\n",
    "                      'MATCH (n) RETURN n LIMIT 100'\n",
    "                      \n",
    "                  100 is arbitrary, but if the default code is ran\n",
    "                  in this notebook, 40 should be enough.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedFromDiskPipe: 0     CleaningPipe: 0     SimiPipe: 0     DBPipe: 0                                                                               \r"
     ]
    }
   ],
   "source": [
    "pipeline = prefabs.get_pipeline_dsk_cln_simi_db(\n",
    "    filepath=PICKLE_FILE,\n",
    "    rec_lvl=1,\n",
    "    \n",
    ").run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
